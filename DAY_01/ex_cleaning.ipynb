{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어 처리를 위한 전처리  \n",
    "- 정제  \n",
    "    * 토큰화 전후로 진행 됨  \n",
    "    * 100 제거는 어려움..  \n",
    "    * 불용어, 빈도에 따른 제거, 길이에 따른 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 모듈로딩\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179개\n",
      "i\n",
      "me my myself we our ours ourselves you you're you've\n",
      "you'll you'd your yours yourself yourselves he him his himself\n",
      "she she's her hers herself it it's its itself they\n",
      "them their theirs themselves what which who whom this that\n",
      "that'll these those am is are was were be been\n",
      "being have has had having do does did doing a\n",
      "an the and but if or because as until while\n",
      "of at by for with about against between into through\n",
      "during before after above below to from up down in\n",
      "out on off over under again further then once "
     ]
    }
   ],
   "source": [
    "# NLTK 제공 영어 불용어 리스트 가져오기\n",
    "english_sw=stopwords.words('english')\n",
    "\n",
    "print(f'{len(english_sw)}개')\n",
    "\n",
    "for idx, _ in enumerate(english_sw[:100]):\n",
    "    print(_, end= ' ' if idx%10 else '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Family in not an important thing Its everything'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 토큰에서 불용어 제거하기\n",
    "text = \"Family in not an important thing. It's everything.\"\n",
    "\n",
    "## 구두점 제거\n",
    "text.replace('.', '')\n",
    "\n",
    "import string\n",
    "punc=string.punctuation\n",
    "for pun in punc:\n",
    "    text=text.replace(pun,'')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['family', 'in', 'not', 'an', 'important', 'thing', 'its', 'everything']\n"
     ]
    }
   ],
   "source": [
    "## 대소문자 통일\n",
    "text=text.lower()\n",
    "\n",
    "# 토큰화 진행\n",
    "toekns=word_tokenize(text)\n",
    "print(toekns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 불용어 제거 : 분석에 의미가 없는 토큰 제거\n",
    "for token in toekns:\n",
    "    if token in english_sw:\n",
    "        # 있다면 제거\n",
    "        toekns.remove(token)\n",
    "    if token in list(punc):\n",
    "        toekns.remove(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family', 'not', 'important', 'thing', 'everything']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toekns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
